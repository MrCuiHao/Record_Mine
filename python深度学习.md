1、小批量随机梯度下降

（1）抽取训练样本x和对应目标y组成的数据批量

（2）在x上运行网络【这一步叫做前向传播】，得到预测值y_pred

（3）计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离

（4）计算损失相对于网络参数的梯度【一次反向传播】

（5）将参数沿着梯度的反方向移动一点，比如w-=step*gradient,从而使这批数据上的损失减小一点。

2、动量

动量在许多优化器中都有应用，动量解决了SGD的两个问题：收敛速度和局部极小点
使用动量方法可以避免陷入局部极小点，动量方法的实现过程是每一步都移动小球，不仅要考虑当前的斜率值（当前的加速度），还要考虑当前的速度
（来自于之前的加速度），这在实践中是指，更新参数w不仅要考虑当前的梯度值，还要考虑上一次的参数更新。

3、局部极小点、全局最小点

在某个参数值附近，有一个局部极小点：在这个点附近向左移动和向右移动都会导致损失值增大。如果使用小学习率的SGD进行优化，那么优化过程可能

会陷入局部极小点，导致无法找到全局最小点。

4、链式求导：反向传播算法

